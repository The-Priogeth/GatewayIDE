# Root‑Kontext — Gebrauch & Regeln
Diese Datei beschreibt für jede Quell‑Datei einen eigenen Kontextblock. Jeder Block beginnt mit einer Überschrift der Form `#backend/<pfad/zur/datei.py>` und enthält einen kompakten Fließtext, der Zweck, Datenfluss, Abhängigkeiten und Änderungsregeln erläutert. Zum Arbeiten suchst du den relevanten Block, liest den Zusammenhang und **überschreibst ausschließlich diesen Block** mit aktualisiertem Fließtext. Keine Hilfsmarker, keine Zitat‑Blöcke, keine losen TODOs. Blöcke dürfen kleiner werden, wenn sich der Umfang der Datei reduziert. Der Chat‑Pfad bleibt auf `autogen_core.memory` und `ZepMemory`; der Orchestrator‑Adapter ist bewusst „thin“ und optional schaltbar. Im Runner‑Container adressierst du Dienste über `http://gateway:8080/…`, auf Host/Server über `http://localhost:8080/…`. bitte den inhalt der blöcke als einzeiler (also ohne Zeilenumbrüche) schreiben.

# .env
Lokale Laufzeitkonfiguration mit Zugängen zu OpenAI-Projekt (API-Key, Project-ID, ORG, Base-URL), Aktivierung des Python-Pfades, Zep-Cloud-Zugängen inkl. stabilen T1–T6 Thread-IDs für SoM-Rollen, sowie Logging-Parametern (Log-Pfad, Rotation/Retention, Diagnose-Flag, Level) und Backend-Chat-Endpoint; ausschließlich lokal gepflegt, niemals committet.

# build-win.bat
build-win.bat Windows-Build-/Run-Skript für GatewayIDE.App; setzt Pfad-Variablen (ROOT, APP, SLN, CSPROJ), definiert Build-Parameter (RUNTIME=win-x64, OUTDIR, OUTEXE) und Fehlerstatus, führt PRECHECKS aus (prüft .NET SDK via dotnet --version, prüft Docker via docker info), entfernt einen ggf. laufenden gateway-container sicher (docker inspect→ID, dann docker rm -f gateway-container), bereinigt frühere Artefakte (%APP%\obj, %APP%\bin), stellt sicher, dass eine Solution existiert (falls %SLN% fehlt → dotnet new sln -n GatewayIDE), verknüpft das UI-Projekt mit der Solution (falls nicht gelistet → dotnet sln add), führt dotnet restore aus und published die Avalonia-App self-contained für win-x64 als SingleFile inkl. nativer Bibliotheken (dotnet publish -c Release -r win-x64 --self-contained true -p:PublishSingleFile=true -p:IncludeNativeLibrariesForSelfExtract=true -o "%OUTDIR%"), meldet „BUILD ERFOLGREICH“ mit Ausgabepfad, startet die erzeugte EXE (GatewayIDE.App.exe) direkt aus dem Zielordner und hält das Fenster offen, besitzt robuste Abbruch- und Endpfade (:ABORT, :END) mit Nutzer-Hinweisen; Ziel: reproduzierbarer One-Click-Build + optionaler Direktstart auf Windows für die Release-Konfiguration.

# pyproject.toml
pyproject.toml enthält alle Metadaten des Backend-Python-Projekts „GatewayIDE“ (Name, Version, Beschreibung, Python-Version ≥3.10), definiert die vollständigen Runtime-Abhängigkeiten für FastAPI-Betrieb, Webserver, File-Watcher, dotenv-Support, Rich-Logging und AG2-basierte Multi-Agent-Architektur inkl. ag2[openai], autogen-agentchat, autogen-core, autogen-ext, zep-cloud, zep-python, und zep-autogen; gewährleistet Kompatibilität zu aktuellen OpenAI-SDK-Versionen und HTTP-Transport durch httpx; dokumentiert optionale AG2-Extras für andere Provider (auskommentiert) sowie ungenutzte Pakete (z. B. Streamlit); definiert zusätzlich einen dev-Extrasnamespace für Linters, Typing-Checks und Tests (pytest, pytest-cov, pytest-asyncio, ruff, mypy), und konfiguriert uv als Paketmanager inkl. Pypi-Index-Quelle — zentrale Stellschraube für reproduzierbare Backend-Builds, Dependency-Upgrades und Sicherheits-Audits.

# uv.lock
Lockfile der Python-Deps; wird vom Resolver gepflegt. (zu groß muss sgeprüft werden wieso)

# deploy/scripts/Dockerfile.ai
Leichtgewichtiges Backend-Image auf python:3.11-slim mit non-interactive Grundsetup und Basispaketen (ca-certificates, tzdata, curl, bash), setzt WORKDIR /app, kopiert zunächst nur pyproject.toml und optional uv.lock zur Cache-Optimierung, installiert pip+uv und synchronisiert das virtuelle Env (uv sync --frozen mit Lockfile-Fallback auf uv sync), wodurch ein .venv unter /app/.venv angelegt wird; setzt PATH auf das .venv und PYTHONPATH=/app, kopiert anschließend den gesamten Quellcode ins Image, exponiert Port 8080, lässt einen optionalen HEALTHCHECK auskommentiert stehen und startet standardmäßig per CMD ["bash","-lc","uvicorn main:app --host 0.0.0.0 --port 8080 --no-access-log"] (kann von Compose überschrieben werden, z. B. auf backend.main:app)

# deploy/gateway-compose.yaml
Docker Compose Orchestrierung mit zwei Services: meganode (Stub auf alpine:3.20, container_name=gateway-meganode, Befehl echo Mega-Node stub up; tail -f /dev/null, restart: unless-stopped) und gateway (Python-Backend, container_name=gateway-container, Build aus Repo-Root ../ mit dockerfile: deploy/scripts/Dockerfile.ai, bindet .env via env_file: ../.env, published Port 8080:8080, restart: no, working_dir: /app, setzt Runtime-Variablen PYTHONPATH=/app, WATCHFILES_FORCE_POLLING=1, PYTHONDONTWRITEBYTECODE=1, mountet das ganze Repo als Live-Volume ../:/app sowie ein persistentes venv-cache Volume auf /app/.venv zur deutlich schnelleren Re-Installation, Command ist ein Bash-Wrap, der beim Start uv sync --frozen || uv sync ausführt und dann uv run uvicorn backend.main:app --host 0.0.0.0 --port 8080 --reload --reload-dir /app/backend --reload-include "*.py" --no-access-log startet (Hot-Reload via watchfiles), wodurch lokale Code-Änderungen direkt greifen; definiert das Volume venv-cache im Compose-Scope, sodass das virtuelle Env build-übergreifend wiederverwendet wird.

# backend/main.py
FastAPI-Entrypoint mit lifespan→bootstrap.ensure_runtime() (legt auf app.state Runtime, zep_client, T1–T6-Thread-IDs, t1_memory/t2_memory, ctx_provider, direkte hma-Instanz, messaging, pbuffer_dir), loggt „Runtime ready“ und startet optional den Datei-Watcher für /app/backend; POST /chat (Model ChatRequest{prompt}) schreibt den User-Prompt als MemoryContent(TEXT) in T1, aktualisiert Kontext, ruft hma.run_inner_cycle(prompt, ctx) (liefert {"ich_text", "route", "inner"}), erzeugt corr_id (uuid4) und messaging.snapshot(...), persistiert inner in T2, bei route.target=="user" persistiert die Assistentenantwort in T1, sonst delegiert kompakt an T4/T5/T6 (Memory-Write mit Meta kind="delegation"), und antwortet mit stabilem Envelope {ok, final=True, deliver_to, speaker="SOM", corr_id=None, packet_id=None, p_snapshot=None, inner, responses}; GET / liefert {status:"ok", message:"Gateway Backend (Slim-HMA) läuft."}.

# backend/bootstrap.py
Lädt .env/ENV (via dotenv) und initialisiert beim ersten Aufruf von ensure_runtime() eine schlanke, wiederverwendbare Runtime als Singleton (SimpleNamespace) bestehend aus: Zep-Async-Client (AsyncZep mit optionalem ZEP_BASE_URL, harter Abbruch ohne ZEP_API_KEY), sechs logisch benannten Zep-Threads T1..T6 samt zugehörigen User-IDs und Memory-Handles (Erzeugung/Absicherung über _ensure_thread(); T1 = dialog/root, T2 = user_visible, T3 = meta_proto, T4 = lib_internal, T5 = task_internal, T6 = trn_internal; Benutzerkennung bevorzugt aus T1_USER_ID/ZEP_USER_ID), einem ContextProvider im summary-Modus (liefert/aktualisiert den kompakten Gesprächskontext aus Zep via thread.get_user_context), einem LLMAdapter (Minimal-Finalizer, extrahiert aus dem SOM-„# Interner Zwischenstand“ heuristisch eine Kurzantwort ≤280 Zeichen und hängt eine Standard-Route {"deliver_to":"user"} an), einem DemoAdapter (vereinheitlicht Aufrufe an AG2-ConversableAgent über .run(user_text, context)), einer kleinen Demo-Flotte aus AG2-Agents (PersonalAgent, DemoTherapist, DemoProgrammer, DemoStrategist, DemoCritic, alle human_input_mode="NEVER"), die in demo_registry als aufrufbare Worker registriert werden, einem optionalen memory_logger für T1 (persistiert eingehende User-Nachrichten als MemoryContent(TEXT); extrahiert robust per Regex „mein Name ist …“ und legt daraus ein kurzes Profil-Snipplet ab; triggert nach jedem Write ctx_provider.refresh()), der HMA-Instanz selbst (über build_hma(demo_registry=..., llm_client=LLMAdapter(...)), also direkt als Objekt, kein Dict), der per-Import bereitgestellten messaging-Schnittstelle, sowie einem (derzeit None) pbuffer_dir-Platzhalter; ensure_runtime() cached das Ergebnis (erneute Aufrufe liefern dasselbe Namespace-Objekt), sodass main.py nach dem Lifespan-Bootstrap synchron auf app.state konsistenten Zugriff auf zep_client, alle t*_thread_id/t*_memory, ctx_provider, hma, messaging und memory_logger hat, während Hilfsklassen/-funktionen oben im Modul (Adapter, _ensure_thread, ContextProvider) ausschließlich als interne Bausteine dienen.

# backend/agent_core/konstruktor.py
Stellt die zentrale Fabrikfunktion build_hma() bereit, die zur Laufzeit eine vollständige HMA-Instanz erzeugt, indem sie den Default-Systemprompt + Templates aus DEFAULT_HMA_CONFIG übernimmt, alle registrierten Demo-Agents als demos einbindet, das Messaging-Modul (backend.agent_core.messaging) dynamisch injiziert und den übergebenen llm_client als LLM-Schnittstelle aktiviert; Einzige Aufgabe: saubere Kapselung der HMA-Konstruktion ohne Logik/State — Frontend für Bootstrap, damit main.py direkt über runtime.hma arbeiten kann.

# backend/agent_core/messaging.py
Definiert ein kompaktes Messaging-Layer für agentenseitige Kommunikation mit zwei Kern-Datentypen Message{role,text,meta,deliver_to} und Envelope{thread,message} plus Hilfsfunktionen: log() zur zentralen Ausgabe (Backend-Logger-Anbindung möglich), store() als Persistenzstub (derzeit reine Ausgabe → später Zep/DB), forward() als bewusst logikfreie Weiterleitungsmarke, snapshot() zur Ablage eines Text-Snapshots im Pufferverzeichnis (UUID-Dateiname, Fallback-Puffer PBUFFER_DIR=/app/pbuffer), sowie log_som_internal_t2() (async) zum Persistieren des kompletten internen SOM-Zwischenstands in Thread T2 via t2_memory.add(...), mit robustem Fehlerhandling und Metaeinträgen (role=system, thread=T2, corr_id) — dient als einheitliche Brücke zwischen HMA, Memory und Debug/Telemetry, ohne geschäftliche Entscheidungen zu treffen.

# backend/agent_core/hma/hma.py
Implementiert den Haupt-Meta-Agenten (HMA) als schlanke SOM-Kernlogik: run_inner_cycle() führt einen internen Zyklus aus (Demo-Auswahl via select_demos, sequentielle Demo-Ausführung _parallel_demo, Ergebnisaggregation aggregate + optionale Kurz-Findings via build_findings), erzeugt aus Templates den Final-Prompt, ruft das LLM via _call_llm() (Systemprompt + user-spezifischer Aggregate-Kontext), extrahiert Routing-Information aus der Modellantwort (parse_deliver_to → Route{target,args}), persistiert interne Beiträge als Telemetrie in Thread T2, und liefert ein kompaktes Objekt {ich_text, route, inner} als Output; Sequenzlogik bewusst minimal, keine Nebenwirkungen außer Logging/Persistierung.

# backend/agent_core/hma/hma_config.py
Stellt die Konfigurationsstruktur HMAConfig bereit (Felder: som_system_prompt, som_plan_template, som_final_template, optionale capabilities, max_parallel_targets) sowie DEFAULT_HMA_CONFIG mit standardisierter Vorgabe: System-Rolle als innere Stimme, Plan-Template inkl. Kontext- und Fähigkeitenblock, Final-Template zur Erzeugung des SOM-Antwortformats inkl. strikter Routenzeile <<<ROUTE>>> {"deliver_to":"user|task|lib|trn","args":{}} <<<END>>> als verpflichtender Abschluss der generierten Assistenzantwort; dient Bootstrap als Default-Injection für den HMA.

# backend/agent_core/hma/routing.py
Kapselt das komplette Routing-Parsing für HMA-Ausgaben: definiert Typen Target("user"|"task"|"lib"|"trn") und Route{target,args}, Regex-Marker zur robusten Routenextraktion (<<<ROUTE>>> {...} <<<END>>>) + Fallback auf das letzte JSON-Objekt mit "deliver_to", eliminiert Markdown-Codezäune, wendet defensive JSON-Parsing-Strategien an (unbekannte Targets → default user), und bietet map_target_to_thread() zur Zuordnung von Routing-Zielen zu HMA-Threads (user→T1, lib→T4, task→T5, trn→T6); sorgt dafür, dass falsche/fehlende Routenzeilen niemals den Dialogfluss crashen.

# backend/agent_core/hma/selectors.py
Mini-Heuristiken zur Demo-Agent-Koordination: select_demos() prüft pro Demo optional vorhandenes accept(user_text,context) und wählt deterministisch eine Agentenmenge (Fallback: alle), aggregate() gruppiert Demo-Antworten zu klar formatierter, redundanzfreier Struktur (## Name\nAntwort-Blöcke), und build_findings() extrahiert Kernbehauptungen aus Demo-Outputs mittels Regex-Patterns (z. B. Name-Claims, Ortsangaben, Handlungsentscheidungen) zu einem kompakten Voting-Block „# Findings (kompakt)“ — dadurch kann HMA implizite Konsenssignale der Demos für seinen Final-Prompt nutzen; reines Daten-Preprocessing ohne Seitenwirkungen.

# backend/memory/memory.py
Implementiert die AutoGen-kompatible Speicher-Fassade ZepMemory(Memory) und komponiert dafür ZepThreadMemory (Thread-Nachrichtenfluss) und ZepGraphAdmin (Graph-CRUD/Search) zu einer sauberen API mit add(), add_episode(), query(), update_context(), clear(), close(); add() akzeptiert ausschließlich MemoryMimeType.{TEXT,MARKDOWN,JSON}, liest content.metadata.type und routet "message" (mit meta.role=user|assistant|system und optional name) in den Thread (add_user_message|add_assistant_message|add_system_message) sowie "data" in den User-Graph (add_raw_data mit Typ-Abbildung text/json), ignoriert Graph-Writes bei lokalen Threads (local_*), adopt_thread_id() setzt extern eine Thread-ID, ensure_thread() liefert die vorhandene ID oder fällt robust auf local_<user_id> zurück, add_episode() kapselt strukturierte JSON-Episoden (kind=episode, Zeitstempel, Quelle/Rolle) über add(), query() führt eine vereinheitlichte Graph-Suche aus (sammelt edges→MemoryContent(edge.fact), nodes→name + summary, episodes→content mit reichhaltigen Metadaten wie edge_name, node_attributes, episode_role usw.) und toleriert Fehler mit Logging, update_context() baut einen kontextuellen Systemblock aus Thread-Context + „Recent conversation“ (bis 10 Nachrichten), versucht best-effort die Injektion als SystemMessage in den ChatCompletionContext und liefert UpdateContextResult, clear() löscht den Thread beim Server (best-effort), close() überlässt den Client-Lifecycle dem Aufrufer; Properties thread_id/user_id dienen Health/Diag, alle Pfade defensiv geloggt.

# backend/memory/memory_zep_graph.py
Stellt ZepGraphAdmin als dünnen, asynchronen Wrapper um die Zep-Graph-API bereit und kapselt Provisioning, CRUD und eine einheitliche search()-Schnittstelle; Kernkonzept ist die Zielauflösung via target_kwargs() (priorisiert graph_id, ansonsten user_id, sonst Fehler) und ein interner _gid()-Helper für strikte ID-Validierung; bereitgestellte Operationen umfassen create_graph/list_graphs/update_graph/clone_graph/set_ontology, add_node (mit optional summary/attributes), add_fact_triple (legt Kanten mit Relation, optionalen Attributen/Rating an), get_node/get_edge/get_node_edges, delete_edge/delete_episode, add_raw_data(user_id|self._user_id, type, data) für freie Datenzufuhr sowie eine generische search(query, limit, scope, search_filters, min_fact_rating, reranker, center_node_uuid, **kwargs), die Ziel- und Abfrageparameter zusammenführt und direkt client.graph.search(**params) ausführt; damit bleibt die Graph-Integration ortsagnostisch (User-Graph oder expliziter Graph) und konsistent erweiterbar.

# backend/memory/memory_zep_thread.py
Implementiert ZepThreadMemory (async) als robuste Thread-Verwaltung mit Lazy-Erzeugung und Wiederanbindung: ensure_thread(force_check) erzeugt bei fehlender ID einen neuen Thread (client.thread.create) und validiert vorhandene IDs optional (get→bei 404 recreate mit gleicher ID), add_user_message/assistant_message/system_message rufen zentral _add_message(role, content, name) auf, das stets eine gültige Thread-ID sicherstellt und bei 404 einmalig rekreiert und dann erneut add_messages ausführt; Lese-/Hilfsfunktionen umfassen list_recent_messages(limit) (wandelt Server-Antworten robust in {role, content, ts} um und schützt lokale/fehlende Threads), search_text(query, limit, roles, exclude_notes, dedupe, max_scan) (scannt die letzten max_scan Nachrichten neu→alt, filtert nach Rollen/„Merke:“/Duplikaten und liefert die ersten limit Treffer), get_user_context(mode|default="basic") (liefert serverseitigen Kontextstring) sowie build_context_block(include_recent=True, recent_limit=10), das einen kompakten Prompt-Block aus Memory context: … und Recent conversation: baut (Nachrichten gekürzt auf 2000 Zeichen, rollennotiert), während is_local/_is_local jede Netz-Interaktion für local_*-Threads verhindert; defensives Logging vorhanden, Fehlerpfade fallen hartlos zurück.

# backend/zep_autogen/exceptions.py
Definiert ZepDependencyError(ImportError) für fehlende AutoGen-/Zep-Abhängigkeiten und formatiert eine klare Installationsanweisung anhand der übergebenen Parameter framework und install_command (kompakt nutzbar als Gate vor Tool-/Memory-Initialisierung).

# backend/zep_autogen/graph_memory.py
Stellt ZepGraphMemory(Memory) als AutoGen-kompatiblen Graph-Speicher bereit (konstruiert mit AsyncZep + graph_id + optionalen SearchFilters, facts_limit, entity_limit), unterstützt add() für TEXT/MARKDOWN/JSON und mapped diese auf Zep-Datentyp (text|json|message), implementiert query() als Wrapper um client.graph.search(...) inkl. defensive Fehlerlogs und konvertiert Treffer (edges|nodes|episodes) konsistent in MemoryContent, ergänzt um _retrieve_graph_context() (letzte Episoden → zwei parallele Graph-Suchen → compose_context_string(...)) und update_context(ChatCompletionContext) (fügt bei Fund einen SystemMessage-Kontext hinzu), sowie clear() (löscht Graph) und close() (Client-Lifecycle bleibt extern); dient explizit dem graph_id-basierten Wissenskontext, getrennt vom Thread-Speicher.

# backend/zep_autogen/memory.py
Liefert ZepUserMemory(Memory) für user_id-gebundene Speicherung und Kontextbereitstellung: add() unterscheidet metadata.type="message" (legt bei Bedarf thread_id an, schreibt via thread.add_messages) und type="data" (schreibt via graph.add in den User-Graph, MIME-Mapping TEXT/MARKDOWN→text, JSON→json), query() ruft client.graph.search(user_id=...) auf und transformiert edges|nodes|episodes in MemoryContent samt Metadaten (z. B. edge_name, node_attributes, episode_role), update_context() zieht thread-basierten Kontext plus letzte Nachrichten und injiziert beides als SystemMessage in den ChatCompletionContext, clear() löscht den Thread (falls vorhanden), close() belässt den Client-Lifecycle extern; damit fokussiert diese Klasse auf den User-Graph + Thread für personengebundene Konversationen.

# backend/zep_autogen/tools.py
Bietet AutoGen-FunctionTools für Zep: search_memory(client, query, graph_id|user_id, limit, scope) durchsucht entweder einen Graphen oder den User-Graph (erzwingt XOR-Parameter), normalisiert Ergebnisse (edges/nodes/episodes) in einfache Dict-Listen, add_graph_data(client, data, graph_id|user_id, data_type) schreibt freie Daten (text|json|message) in Graph oder User-Graph, und die Fabriken create_search_graph_tool(...)/create_add_graph_data_tool(...) erzeugen vorkonfigurierte Tools mit gebundenem AsyncZep und Scope (Graph vs. User) inkl. klarer ValueError-Guards; dient der schnellen Tool-Integration von Zep-Suche/Write in Agent-Flows.

# GatewayIDE.App/App.axaml
XAML-Deklaration der Anwendung (Klasse GatewayIDE.App.App) mit gesetztem FluentTheme-Paket der Avalonia-UI-Bibliothek (Standard-FluentTheme ohne weitere Style-Overrides) und global aktivierter RequestedThemeVariant="Dark", keine globalen Ressourcen, Event-Trigger oder Datenbindungen — reiner Theme- und Ressourcen-Anker für das gesamte Frontend.

# GatewayIDE.App/App.axaml.cs
Code-behind zur App-Initialisierung mit robustem Fehlertracking (registriert globale Handler für AppDomain.CurrentDomain.UnhandledException und TaskScheduler.UnobservedTaskException → loggt ungefangene Fehler nach GatewayIDE-crash.log), lädt die XAML-Ressourcen via AvaloniaXamlLoader.Load(this) im Konstruktor/Initialize-Pfad, und erzeugt im Desktop-Lifetime (Windowed-Umgebung) die MainWindow-Instanz mit zugehörigem MainWindowViewModel als DataContext, bevor der Lifetime-Start abgeschlossen wird — garantiert sauberen UI-Start trotz möglicher interner Fehler

# GatewayIDE.App/Converters.cs
Enthält den UI-ValueConverter HalfConverter : IValueConverter, der im Convert-Pfad bei gültigen double-Werten die Hälfte des Eingabewerts berechnet und gleichzeitig eine Mindestgröße von 48 px durch Math.Max(48, d * 0.5) garantiert (bei ungültigen Werten Rückfall auf 200d), während ConvertBack nicht unterstützt ist (wirft NotSupportedException); Einsatzgebiet sind Layout-Bindings, die dynamische Größen responsiv halbieren, ohne unter eine sinnvolle Mindestbreite/Höhe zu fallen.

# GatewayIDE.App/GatewayIDE.App.csproj
MSBuild-Projektdatei für die Avalonia-Desktop-App (Target net8.0, OutputType=WinExe, Nullable/ImplicitUsings aktiv, AssemblyName GatewayIDE.App) mit einem konsolidierten Paketblock für UI (Avalonia, Avalonia.Desktop, Avalonia.ReactiveUI, Avalonia.Themes.Fluent jeweils 11.1.3) und gRPC-Clientunterstützung (Grpc.Tools 2.63.0 als PrivateAssets=All, Grpc.Net.Client 2.63.0, Google.Protobuf 3.25.3), räumt zunächst alle .axaml-Einträge aus den Standard-Itemgruppen und bindet die benötigten XAMLs explizit wieder ein (App.axaml, MainWindow.axaml), inkludiert statische Assets aus Assets\**, und richtet die gRPC-Clientgenerierung gegen die Solution-Root ein (Protobuf-Include via ..\Protos\ai_service.proto, GrpcServices="Client", ProtoRoot="..\Protos"), womit das Projekt schlank bleibt (keine doppelten Paketblöcke), nur gewünschte XAMLs in den Build gelangen und bei Bedarf ein generierter gRPC-Client für ai_service.proto bereitsteht.

# GatewayIDE.App/MainWindow.axaml
Deklaration des Hauptfensters GatewayIDE.App.MainWindow (1280×800, Titel „Gateway IDE“) mit einem Root-Grid (Zeilen: Auto,*) für eine dunkle Top-Bar und den Inhalt, einer Topbar (44px, Spalten 44,*,Auto,Auto,Auto,Auto,Auto) mit Menü-Toggle (ToggleChatCommand) und fünf Tab-Buttons (Dashboard, Docker, KI System, Project, Blockchain jeweils über SelectTabCommand mit CommandParameter), gefolgt von einem zweispaltigen Content-Bereich: links eine Chat-Sidebar (ListBox ChatLines inkl. ItemTemplate als read-only TextBox mit Monospace-Font, plus Eingabe ChatInput mit Enter-KeyBinding → SendChatCommand), rechts der Main-Workspace als Grid mit drei Tab-Sektionen, die jeweils über IsDashboard/IsDocker/IsKiSystem sichtbar geschaltet werden; die Dashboard-Ansicht teilt sich in einen oberen Bereich (links Button-WrapPanel mit „Start Mega-Node“, rechts ein Status-Panel mit gebundenen Text/Brush-Feldern DockerDesktopStatus, DockerImageStatus, DockerContainerStatus) und ein unteres Terminal (read-only TextBox TerminalBuffer), die Docker-Ansicht bietet eine Toolbar mit Aktionsbuttons (Rebuild, Start, Stop, Remove container, Clear Logs, sowie ExpandGatewayOnlyCommand mit ExpandLabel) und ein zweiteiliges Ober-Grid (oben links DockerOutBuffer, oben rechts DockerErrBuffer, darunter vollbreit GatewayLogBuffer) plus ein Unter-Grid mit Output (ContainerIOBuffer) und mehrzeiliger Input-Box (ContainerCommand → Enter triggert ExecuteInContainerCommand), und die KI-System-Ansicht strukturiert ein 2×2-Board für T2/T4/T5/T6-Textfelder (T2Buffer, T4Buffer, T5Buffer, T6Buffer) mit Monospace-Anzeige, dazu rechte Platzhalterspalten (3 Felder) und einen unteren Platzhalterbereich; sämtliche Textboxen sind auf Wrap/Readonly/Scroll ausgelegt, viele Felder binden zusätzlich CaretIndex-Properties für Auto-Scroll, wodurch das Fenster als schlanker Mission-Control-Container für Chat, Docker-Steuerung/Logs und SOM-Thread-Telemetrie fungiert.

# GatewayIDE.App/MainWindow.axaml.cs
Code-behind des Hauptfensters mit minimalem Bootstrap: deklariert die MainWindow-Klasse (partial) im Namespace GatewayIDE.App und ruft im parameterlosen Konstruktor ausschließlich InitializeComponent() auf, wodurch die in MainWindow.axaml definierten Steuerelemente und Bindings geladen werden; keine zusätzliche UI-Logik, keine Events, dient rein der XAML-Initialisierung.

# GatewayIDE.App/Program.cs
Einstiegspunkt der Desktop-App (STAThread) und Avalonia-Bootstrap: Main(string[] args) ruft BuildAvaloniaApp().StartWithClassicDesktopLifetime(args) auf, BuildAvaloniaApp() konfiguriert den AppBuilder für App, aktiviert .UsePlatformDetect() (plattformabhängige Backends) und .LogToTrace() (einfaches Tracing-Logging), keine Custom-Args-Auswertung oder DI-Konfiguration; Ziel ist ein schlanker, plattformneutraler Start der Avalonia-Anwendung.

# GatewayIDE.App/Services/AI/AIClientService.cs
gRPC-Client-Wrapper für den vom Backend generierten AI-Service-Client (Gateway.AI.V1.AIService.AIServiceClient), erzeugt in AIClientService(string endpoint) einen GrpcChannel zu einer konfigurierten Endpoint-URL, bietet aktuell eine Beispiel-API EchoAsync(text) zur asynchronen Roundtrip-Kommunikation über den gRPC-EchoAsync-Endpoint, und übernimmt über DisposeAsync() das saubere Schließen des gRPC-Channels (kein ShutdownAsync nötig, da GrpcChannel.Dispose() genügt) — zentrale Schnittstelle, um Backend-KI-Funktionen aus dem UI anzusprechen.

# GatewayIDE.App/Services/Processes/DockerService.cs
statischer High-Level-Orchestrator für Docker-Operationen, inklusive automatischer Ermittlung des Repo-Root-Pfads (FindRepoRoot), Caching der deploy-Ablage, und eines robusten, asynchronen Low-Level-Prozess-Runners (RunAsync) mit Echtzeit-Streaming von stdout/stderr (für Terminal-UI), mit Status-Abfragen zu Docker Desktop (inkl. Windows-Service-Fallback), Kommandos für Container-Lifecycle (StartGatewayAsync, StopGatewayAsync, RemoveGatewayContainerAsync, GetGatewayStatusAsync), Image-Checks (IsImageAvailableAsync), Full-Wipe & Rebuild-Pipeline (WipeAllAsync, FullRebuildAsync), Log-Streaming (TailGatewayLogsAsync) und direkter Exec-Bridge in den Container (ExecInGatewayAsync mit Bash-Shell); dient als vollständig UI-steuerbarer Control-Tower für den Gateway-Backend-Container im Entwicklungsbetrieb.

# GatewayIDE.App/Services/Processes/ProcessManager.cs
minimalistischer, generischer Prozessstarter (StartProcess(file, args, cwd?)) für externe Tools (z. B. eigene CLI-Tests), setzt WorkingDirectory, deaktiviert Shell-Ausführung, aktiviert Output-Redirect für spätere UI-Log-Integration, keine asynchrone Steuerung oder Lifecycle-Events — dient vorrangig als Legacy-Fallback gegenüber den strukturierten Docker-Routinen im DockerService.

# GatewayIDE.App/ViewModels/DelegateCommand.cs
ist ein kompakter ICommand-Wrapper zur Bindung von UI-Interaktionen, kapselt Rückrufe über _execute(object?) und optional _canExecute(object?), implementiert CanExecuteChanged und bietet RaiseCanExecuteChanged() zur UI-Aktualisierung — universeller, UI-agnostischer Command-Adapter für Buttons, Keyboard-Bindings usw., Grundlage aller ViewModel-Aktionen.

# GatewayIDE.App/ViewModels/MainWindowViewModel.cs
Haupt-ViewModel der gesamten IDE-Oberfläche: steuert Tab-Navigation und Sichtbarkeit (Dashboard, Docker, KI System), verwaltet die linke Chat-Sidebar inkl. Verlauf (ChatLines), Eingabe (ChatInput), Highlighting/Autoscroll, führt asynchrone Chat-Requests via HttpClient → POST /chat aus und verteilt Antworten kontextabhängig auf HMA-Threads (SOM→T1, SOM:INNER→T2, TASKMANAGER→T4, LIBRARIAN→T5, TRAINER→T6, RETURN→T3), trackt Systemstatus (DockerDesktopStatus, DockerImageStatus, DockerContainerStatus + Brush-Bindings), bietet umfangreiche Log-Buffer (Docker stdout/stderr + Gateway-Logs + Container-IO) und Auto-Scroll mittels Caret-Bindings, kontrolliert UI-Layout im Docker-Tab (Grid-Shrink/Expand über ExpandGatewayOnlyCommand, bindbare GridLength-Properties), enthält Full-Lifecycle-Bridges zu Docker (Rebuild, Start, Stop, Remove, Exec) über DockerService, hält Absicherungen gegen Log-Prozess-Leaks (_gatewayTailCts, _meganodeLogsProc), sowie Utility-Funktionen (Clear-Logs, Repo-Pfad-Suche, Prozess-Attach-Pipelines) — damit zentrale Mission-Control-Steuereinheit zwischen UI, System-Runtime (Docker) und HMA-Backend-Interaktion.

# protos/ai_service.proto
gRPC-Proto für den AI-Service mit einem AIService-Contract, der zwei RPC-Methoden definiert: Echo(EchoRequest) → EchoReply für synchrone Text-Roundtrips und ChatStream(stream ChatClientMsg) → stream ChatServerMsg für bidirektionale Streaming-Dialoge, jeweils mit einfachem Nachrichten-Schema (string text), dient als Grundlage zur Codegenerierung für Client (UI) und Server (Backend/Tests), wird in Visual Studio per GatewayIDE.App.csproj automatisch eingebunden und beim Build generiert. (Ich brauche später noch deinen exakten Proto-Inhalt; falls du möchtest, ergänze ich Field-IDs, Namespaces & Imports sobald vorliegend.)

# services/GatewayAI/ai_service/requirements.txt
Minimales Python-SDK-Abhängigkeitsset für eine gRPC-Service-Implementierung (grpcio==1.65.0, grpcio-tools==1.65.0), optional als Entwicklungs-/Generatorgrundlage für ai_service_pb2*.py, löst weder FastAPI-Backend noch Uvicorn aus — dedizierte, abgespeckte Runtimeschicht für Tests oder lokale Microservices.

# services/GatewayAI/ai_service/server.py
Leichter Async-gRPC-Testserver: setzt eigenen Modulpfad für generierten Code (sys.path.append(BASE)), lädt Proto-Stubs (ai_service_pb2, ai_service_pb2_grpc), implementiert AIServiceImpl mit Echo-Funktion (Echo → "echo:<input>") und bidirektionalem Chat-Stream (für jede eingehende ChatClientMsg wird "bot:<input>" zurückgegeben), startet den Server via rpc.add_AIServiceServicer_to_server auf Port 50051 (await server.start() + await wait_for_termination()), wird als eigenständiges Debug-Tool über python server.py ausgeführt — Test-Doppel für Frontend-Integration ohne komplettes Backend.





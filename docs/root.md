# Root‑Kontext — Gebrauch & Regeln
Diese Datei beschreibt für jede Quell‑Datei einen eigenen Kontextblock. Jeder Block beginnt mit einer Überschrift der Form `#backend/<pfad/zur/datei.py>` und enthält einen kompakten Fließtext, der Zweck, Datenfluss, Abhängigkeiten und Änderungsregeln erläutert. Zum Arbeiten suchst du den relevanten Block, liest den Zusammenhang und **überschreibst ausschließlich diesen Block** mit aktualisiertem Fließtext. Keine Hilfsmarker, keine Zitat‑Blöcke, keine losen TODOs. Blöcke dürfen kleiner werden, wenn sich der Umfang der Datei reduziert. Der Chat‑Pfad bleibt auf `autogen_core.memory` und `ZepMemory`; der Orchestrator‑Adapter ist bewusst „thin“ und optional schaltbar. Im Runner‑Container adressierst du Dienste über `http://gateway:8080/…`, auf Host/Server über `http://localhost:8080/…`. bitte den inhalt der blöcke als einzeiler (also ohne Zeilenumbrüche) schreiben.

# .env
Lokale Laufzeitkonfiguration mit Zugängen zu OpenAI-Projekt (API-Key, Project-ID, ORG, Base-URL), Aktivierung des Python-Pfades, Zep-Cloud-Zugängen inkl. stabilen T1–T6 Thread-IDs für SoM-Rollen, sowie Logging-Parametern (Log-Pfad, Rotation/Retention, Diagnose-Flag, Level) und Backend-Chat-Endpoint; ausschließlich lokal gepflegt, niemals committet.

# build-win.bat
build-win.bat Windows-Build-/Run-Skript für GatewayIDE.App; setzt Pfad-Variablen (ROOT, APP, SLN, CSPROJ), definiert Build-Parameter (RUNTIME=win-x64, OUTDIR, OUTEXE) und Fehlerstatus, führt PRECHECKS aus (prüft .NET SDK via dotnet --version, prüft Docker via docker info), entfernt einen ggf. laufenden gateway-container sicher (docker inspect→ID, dann docker rm -f gateway-container), bereinigt frühere Artefakte (%APP%\obj, %APP%\bin), stellt sicher, dass eine Solution existiert (falls %SLN% fehlt → dotnet new sln -n GatewayIDE), verknüpft das UI-Projekt mit der Solution (falls nicht gelistet → dotnet sln add), führt dotnet restore aus und published die Avalonia-App self-contained für win-x64 als SingleFile inkl. nativer Bibliotheken (dotnet publish -c Release -r win-x64 --self-contained true -p:PublishSingleFile=true -p:IncludeNativeLibrariesForSelfExtract=true -o "%OUTDIR%"), meldet „BUILD ERFOLGREICH“ mit Ausgabepfad, startet die erzeugte EXE (GatewayIDE.App.exe) direkt aus dem Zielordner und hält das Fenster offen, besitzt robuste Abbruch- und Endpfade (:ABORT, :END) mit Nutzer-Hinweisen; Ziel: reproduzierbarer One-Click-Build + optionaler Direktstart auf Windows für die Release-Konfiguration.

# pyproject.toml
pyproject.toml enthält alle Metadaten des Backend-Python-Projekts „GatewayIDE“ (Name, Version, Beschreibung, Python-Version ≥3.10), definiert die vollständigen Runtime-Abhängigkeiten für FastAPI-Betrieb, Webserver, File-Watcher, dotenv-Support, Rich-Logging und AG2-basierte Multi-Agent-Architektur inkl. ag2[openai], autogen-agentchat, autogen-core, autogen-ext, zep-cloud, zep-python, und zep-autogen; gewährleistet Kompatibilität zu aktuellen OpenAI-SDK-Versionen und HTTP-Transport durch httpx; dokumentiert optionale AG2-Extras für andere Provider (auskommentiert) sowie ungenutzte Pakete (z. B. Streamlit); definiert zusätzlich einen dev-Extrasnamespace für Linters, Typing-Checks und Tests (pytest, pytest-cov, pytest-asyncio, ruff, mypy), und konfiguriert uv als Paketmanager inkl. Pypi-Index-Quelle — zentrale Stellschraube für reproduzierbare Backend-Builds, Dependency-Upgrades und Sicherheits-Audits.

# uv.lock
Lockfile der Python-Deps; wird vom Resolver gepflegt. (zu groß muss sgeprüft werden wieso)

# deploy/scripts/Dockerfile.ai
Leichtgewichtiges Backend-Image auf python:3.11-slim mit non-interactive Grundsetup und Basispaketen (ca-certificates, tzdata, curl, bash), setzt WORKDIR /app, kopiert zunächst nur pyproject.toml und optional uv.lock zur Cache-Optimierung, installiert pip+uv und synchronisiert das virtuelle Env (uv sync --frozen mit Lockfile-Fallback auf uv sync), wodurch ein .venv unter /app/.venv angelegt wird; setzt PATH auf das .venv und PYTHONPATH=/app, kopiert anschließend den gesamten Quellcode ins Image, exponiert Port 8080, lässt einen optionalen HEALTHCHECK auskommentiert stehen und startet standardmäßig per CMD ["bash","-lc","uvicorn main:app --host 0.0.0.0 --port 8080 --no-access-log"] (kann von Compose überschrieben werden, z. B. auf backend.main:app)

# deploy/gateway-compose.yaml
Docker Compose Orchestrierung mit zwei Services: meganode (Stub auf alpine:3.20, container_name=gateway-meganode, Befehl echo Mega-Node stub up; tail -f /dev/null, restart: unless-stopped) und gateway (Python-Backend, container_name=gateway-container, Build aus Repo-Root ../ mit dockerfile: deploy/scripts/Dockerfile.ai, bindet .env via env_file: ../.env, published Port 8080:8080, restart: no, working_dir: /app, setzt Runtime-Variablen PYTHONPATH=/app, WATCHFILES_FORCE_POLLING=1, PYTHONDONTWRITEBYTECODE=1, mountet das ganze Repo als Live-Volume ../:/app sowie ein persistentes venv-cache Volume auf /app/.venv zur deutlich schnelleren Re-Installation, Command ist ein Bash-Wrap, der beim Start uv sync --frozen || uv sync ausführt und dann uv run uvicorn backend.main:app --host 0.0.0.0 --port 8080 --reload --reload-dir /app/backend --reload-include "*.py" --no-access-log startet (Hot-Reload via watchfiles), wodurch lokale Code-Änderungen direkt greifen; definiert das Volume venv-cache im Compose-Scope, sodass das virtuelle Env build-übergreifend wiederverwendet wird.

# backend/main.py
FastAPI-Entrypoint des Slim-HMA-Backends. Das Modul definiert eine lifespan()-Funktion, die über bootstrap.ensure_runtime() die komplette Runtime initialisiert (Zep-Client, Threads T1–T6, ZepMemory, HMA, Messaging und ContextProvider) und alle Handles im app.state bereitstellt. Während des Starts werden Thread-IDs und Memory-Instanzen geloggt und ein optionaler Dateiwächter (start_watcher("/app/backend")) für Hot-Reload aktiviert. Die globale FastAPI-App wird mit Titel und Version initialisiert und bindet den Chat-Router (backend.routes.chat_api.router) ein, welcher den Endpoint POST /chat bereitstellt (Empfang des User-Prompts, Kontextabruf über ZepMemory.get_context(), Ausführung von hma.run_inner_cycle(), Persistenz über Speaker und Rückgabe eines stabilen Envelope-JSON für die UI). Zusätzlich existiert GET / als Health-Check, der den Status {status:"ok", message:"Gateway Backend (Slim-HMA) läuft."} liefert. Die Lifespan-Funktion räumt beim Shutdown sauber auf und meldet den Abschluss im Logger; alle Operationen sind asynchron und auf eine stabile Runtime-Kohärenz ausgelegt.

# backend/bootstrap.py
Initialisiert beim ersten Aufruf von ensure_runtime() die vollständige Slim-HMA-Runtime als wiederverwendbares Singleton (SimpleNamespace) und lädt .env/ENV-Konfiguration (u. a. ZEP-API-Key, Model-Name); erstellt einen AsyncZep-Client, erzeugt und registriert alle sechs Named-Threads T1..T6 (via _ensure_thread() inkl. garantierter User-IDs, Thread-IDs und ZepMemory-Instanzen für Persistenz), richtet einen schlanken ContextProvider ein (summary-Modus, liefert kompakten Dialogkontext aus Zep), führt eine vollständige AG2-Demo-Flotte (PersonalAgent, DemoTherapist, DemoProgrammer, DemoStrategist, DemoCritic – alle human_input_mode=NEVER) in einer Demo-Registry, bietet optional einen Memory-Logger (schreibt eingehende User-Nachrichten in T1 und extrahiert rudimentäre Profil-Signale), erstellt den LLMAdapter (Initialmodell aus ENV) und erzeugt mittels build_hma(runtime, llm_client, demo_registry, DEFAULT_HMA_CONFIG) genau eine HMA-Instanz; schreibt alle relevanten Handles in runtime.* (u. a. t_thread_id, t_memory, hma, messaging, ctx_provider) und sichert sie global, sodass FastAPI-Lifespan und Chat-API stabil darauf zugreifen können.

# backend/agent_core/konstruktor.py
Beinhaltet die Fabrikfunktion build_hma(runtime, llm_client, demo_registry, config), die den HMA vollständig zusammenstellt: übergibt som_system_prompt und Templates aus HMAConfig, setzt die Demo-Registry deterministisch als parallele Antwort-Quellen, bindet runtime.messaging für Logging/Snapshots sowie den Speaker für Persistenz & Routing ein, injiziert ausschließlich den übergebenen llm_client für Modellzugriffe und gibt eine zustandsarme, sofort lauffähige HMA-Instanz zurück; Zweck: einziger legaler Konstruktionspfad für den HMA, klar getrennt von Bootstrap-Konfiguration und Ausführungslogik.

# backend/agent_core/messaging.py
Definiert das kompakte Messaging-Subsystem zwischen HMA, Speaker und Persistenz: liefert die Basistypen Message{role,text,meta,deliver_to?} und Envelope{thread,message}, bietet log() als zentralen Logger-Hook, store() als optionale Persistenz-Bridge (zukunftssicher zu DB/Zep), forward() als logikfreie Routing-Marke, snapshot() zur Dateiablage größerer Debug-Blöcke (UUID-basiert, Pfad via ENV PBUFFER_DIR), und log_som_internal_t2(...) als asynchronen T2-Persist-Pfad für vollständige SOM-Zwischenstände (# Interner Zwischenstand + # Ich-Antwort (Roh)), inkl. Metadaten (role=system, thread=T2, corr_id) und robuster Fehlerprotokollierung; Messaging entscheidet nichts fachlich, sondern garantiert, dass Debug-/Persistenz-Spuren in T2 durchgehend korrekt sind.

# backend/agent_core/hma/hma.py
Implementiert den Haupt-Meta-Agenten (HMA) als schlanke SOM-Kernlogik: run_inner_cycle() führt Auswahl und parallele Ausführung der Demo-Agenten durch (über selectors.select_demos/_parallel_demo), aggregiert ihre Beiträge strukturiert (mit klarer Rollenkennzeichnung) und reichert sie optional mit Kurz-Findings (selectors.build_findings) an; erzeugt aus Systemprompt+Kontext+Aggregaten einen Final-Prompt (som_final_template), ruft das LLM über einen injizierten Client an, extrahiert aus der Modellantwort den finalen Ich-Text und eine Route (routing.parse_deliver_to), kapselt die internen Ergebnisse kompakt in inner und liefert {ich_text, route, inner} an den Speaker zurück; keine direkte Persistenz im HMA, keine UI-Logik – reine Denk- und Antwortlogik der inneren Stimme.

# backend/agent_core/hma/hma_config.py
Stellt die Konfigurationsstruktur HMAConfig bereit (Felder: som_system_prompt, som_plan_template, som_final_template, optionale capabilities, max_parallel_targets) sowie DEFAULT_HMA_CONFIG mit standardisierter Vorgabe: System-Rolle als innere Stimme, Plan-Template inkl. Kontext- und Fähigkeitenblock, Final-Template zur Erzeugung des SOM-Antwortformats inkl. strikter Routenzeile <<<ROUTE>>> {"deliver_to":"user|task|lib|trn","args":{}} <<<END>>> als verpflichtender Abschluss der generierten Assistenzantwort; dient Bootstrap als Default-Injection für den HMA.

# backend/agent_core/hma/routing.py
Bietet das zentrale und robuste Routing-Subsystem für den HMA: definiert Zieltypen (user/lib/task/trn) und Route{target,args}, extrahiert Routings zuverlässig aus dem vom LLM erzeugten Antwortformat über parse_deliver_to (Marker <<<ROUTE>>> {...} <<<END>>> oder Fallback auf das letzte JSON-Objekt mit deliver_to), entfernt Formatierungsartefakte (Markdown-Zäune etc.), validiert und normalisiert Ziele defensiv (unbekannt → user), und mappt sie über map_target_to_thread() eindeutig auf HMA-Threads (user→T1, lib→T4, task→T5, trn→T6); dieses Modul ist die Single Source of Truth für Zielkonzepte und Thread-Zuordnung im gesamten Backend.

# backend/agent_core/hma/selectors.py
Enthält die Selektions- und Vorverarbeitungslogik für Demo-Agenten: select_demos() überprüft pro Demo optional vorhandene accept(user_text,context)-Heuristiken und wählt deterministisch eine reduzierte Agentenmenge (Fallback: alle), _parallel_demo() ruft diese Demos gereinigt und konsistent auf, aggregate() führt die Resultate in ein strukturiertes, redundanzarmes Format zusammen (Abschnitte ## <Agentenname>), und build_findings() extrahiert gemeinsame Aussagen, Entitäten und Entscheidungen aus den Outputs in einem kurzen Voting-Block („# Findings (kompakt)“) zur besseren Entscheidungsgrundlage für das SOM-Finale; ausschließlich Preprocessing ohne Persistenz oder Zugriff auf Routing/UI.

# backend/agent_core/hma/speaker.py
Definiert den Speaker als Ausgabe- und Persistenzschicht des HMA. Er nimmt nach hma.run_inner_cycle() die generierte Antwort entgegen, analysiert die Routeninformation (route.target) über routing.map_target_to_thread() und persistiert zunächst den kompletten inneren SOM-Zwischenstand in Thread T2 (t2_memory.add(...)) für Telemetrie und Debug. Je nach Ziel wird die finale Antwort im passenden Thread gespeichert (T1 für user, T4/T5/T6 für lib/task/trn Delegationen). Alle Writes laufen über ZepMemory.add_message() bzw. die Thread-Fassade und erzeugen eine vollständige Envelope-Struktur mit corr_id, Meta-Informationen und Snapshot-Referenz. Damit stellt der Speaker sicher, dass sämtliche Kommunikation zwischen HMA, Memory und UI über einen konsistenten Pfad verläuft und keine anderen Module direkt in Threads schreiben.

# backend/memory/memory.py
Implementiert die einheitliche Speicher-Fassade ZepMemory(Memory) als AutoGen-kompatible Schnittstelle, welche Thread-Nachrichten (über ZepThreadMemory) und Wissensfakten (über ZepGraphAdmin) zu einem kohärenten Speicher verbindet. Die Klasse bietet die öffentliche API add(), add_episode(), query(), update_context(), clear(), close() sowie ergänzende Komfortmethoden adopt_thread_id(), adopt_graph_id(), get_context() und add_message(). add() unterscheidet zwischen message (user/assistant/system → Thread) und data (JSON/text → Graph) und schützt lokale Threads vor Graph-Writes. add_episode() legt strukturierte Episoden mit Zeitstempel als JSON im User-Graph ab. query() führt eine kombinierte Graph-Suche aus und formatiert Edges, Nodes und Episodes zu MemoryContent-Objekten. update_context() baut einen kontextuellen System-Block aus Thread-Context und den letzten n Nachrichten auf (Injection in ChatCompletionContext) und liefert ein UpdateContextResult. get_context() stellt denselben Kontext als reinen String bereit (optional inkl. kompaktem Graph-Snippet), während add_message() als Standard-Write-Pfad für Speaker/Demos dient (Thread-first, optional auch Graph). Fehlerpfade sind defensiv geloggt, IDs und Health-Daten sind als Properties exponiert; damit bildet ZepMemory die allein gültige Persistenz-Ebene zwischen HMA, Speaker und Zep-Backend.

# backend/memory/memory_zep_graph.py
Stellt ZepGraphAdmin als dünnen, asynchronen Wrapper um die Zep-Graph-API bereit und kapselt Provisioning, CRUD und eine einheitliche search()-Schnittstelle; Kernkonzept ist die Zielauflösung via target_kwargs() (priorisiert graph_id, ansonsten user_id, sonst Fehler) und ein interner _gid()-Helper für strikte ID-Validierung; bereitgestellte Operationen umfassen create_graph/list_graphs/update_graph/clone_graph/set_ontology, add_node (mit optional summary/attributes), add_fact_triple (legt Kanten mit Relation, optionalen Attributen/Rating an), get_node/get_edge/get_node_edges, delete_edge/delete_episode, add_raw_data(user_id|self._user_id, type, data) für freie Datenzufuhr sowie eine generische search(query, limit, scope, search_filters, min_fact_rating, reranker, center_node_uuid, **kwargs), die Ziel- und Abfrageparameter zusammenführt und direkt client.graph.search(**params) ausführt; damit bleibt die Graph-Integration ortsagnostisch (User-Graph oder expliziter Graph) und konsistent erweiterbar.

# backend/memory/memory_zep_thread.py
Implementiert ZepThreadMemory (async) als robuste Thread-Verwaltung mit Lazy-Erzeugung und Wiederanbindung: ensure_thread(force_check) erzeugt bei fehlender ID einen neuen Thread (client.thread.create) und validiert vorhandene IDs optional (get→bei 404 recreate mit gleicher ID), add_user_message/assistant_message/system_message rufen zentral _add_message(role, content, name) auf, das stets eine gültige Thread-ID sicherstellt und bei 404 einmalig rekreiert und dann erneut add_messages ausführt; Lese-/Hilfsfunktionen umfassen list_recent_messages(limit) (wandelt Server-Antworten robust in {role, content, ts} um und schützt lokale/fehlende Threads), search_text(query, limit, roles, exclude_notes, dedupe, max_scan) (scannt die letzten max_scan Nachrichten neu→alt, filtert nach Rollen/„Merke:“/Duplikaten und liefert die ersten limit Treffer), get_user_context(mode|default="basic") (liefert serverseitigen Kontextstring) sowie build_context_block(include_recent=True, recent_limit=10), das einen kompakten Prompt-Block aus Memory context: … und Recent conversation: baut (Nachrichten gekürzt auf 2000 Zeichen, rollennotiert), während is_local/_is_local jede Netz-Interaktion für local_*-Threads verhindert; defensives Logging vorhanden, Fehlerpfade fallen hartlos zurück.

# backend/zep_autogen/exceptions.py
Definiert ZepDependencyError(ImportError) für fehlende AutoGen-/Zep-Abhängigkeiten und formatiert eine klare Installationsanweisung anhand der übergebenen Parameter framework und install_command (kompakt nutzbar als Gate vor Tool-/Memory-Initialisierung).

# backend/zep_autogen/graph_memory.py
Stellt ZepGraphMemory(Memory) als AutoGen-kompatiblen Graph-Speicher bereit (konstruiert mit AsyncZep + graph_id + optionalen SearchFilters, facts_limit, entity_limit), unterstützt add() für TEXT/MARKDOWN/JSON und mapped diese auf Zep-Datentyp (text|json|message), implementiert query() als Wrapper um client.graph.search(...) inkl. defensive Fehlerlogs und konvertiert Treffer (edges|nodes|episodes) konsistent in MemoryContent, ergänzt um _retrieve_graph_context() (letzte Episoden → zwei parallele Graph-Suchen → compose_context_string(...)) und update_context(ChatCompletionContext) (fügt bei Fund einen SystemMessage-Kontext hinzu), sowie clear() (löscht Graph) und close() (Client-Lifecycle bleibt extern); dient explizit dem graph_id-basierten Wissenskontext, getrennt vom Thread-Speicher.

# backend/zep_autogen/memory.py
Liefert ZepUserMemory(Memory) für user_id-gebundene Speicherung und Kontextbereitstellung: add() unterscheidet metadata.type="message" (legt bei Bedarf thread_id an, schreibt via thread.add_messages) und type="data" (schreibt via graph.add in den User-Graph, MIME-Mapping TEXT/MARKDOWN→text, JSON→json), query() ruft client.graph.search(user_id=...) auf und transformiert edges|nodes|episodes in MemoryContent samt Metadaten (z. B. edge_name, node_attributes, episode_role), update_context() zieht thread-basierten Kontext plus letzte Nachrichten und injiziert beides als SystemMessage in den ChatCompletionContext, clear() löscht den Thread (falls vorhanden), close() belässt den Client-Lifecycle extern; damit fokussiert diese Klasse auf den User-Graph + Thread für personengebundene Konversationen.

# backend/zep_autogen/tools.py
Bietet AutoGen-FunctionTools für Zep: search_memory(client, query, graph_id|user_id, limit, scope) durchsucht entweder einen Graphen oder den User-Graph (erzwingt XOR-Parameter), normalisiert Ergebnisse (edges/nodes/episodes) in einfache Dict-Listen, add_graph_data(client, data, graph_id|user_id, data_type) schreibt freie Daten (text|json|message) in Graph oder User-Graph, und die Fabriken create_search_graph_tool(...)/create_add_graph_data_tool(...) erzeugen vorkonfigurierte Tools mit gebundenem AsyncZep und Scope (Graph vs. User) inkl. klarer ValueError-Guards; dient der schnellen Tool-Integration von Zep-Suche/Write in Agent-Flows.

# GatewayIDE.App/App.axaml
XAML-Deklaration der Anwendung (Klasse GatewayIDE.App.App) mit gesetztem FluentTheme-Paket der Avalonia-UI-Bibliothek (Standard-FluentTheme ohne weitere Style-Overrides) und global aktivierter RequestedThemeVariant="Dark", keine globalen Ressourcen, Event-Trigger oder Datenbindungen — reiner Theme- und Ressourcen-Anker für das gesamte Frontend.

# GatewayIDE.App/App.axaml.cs
Code-behind zur App-Initialisierung mit robustem Fehlertracking (registriert globale Handler für AppDomain.CurrentDomain.UnhandledException und TaskScheduler.UnobservedTaskException → loggt ungefangene Fehler nach GatewayIDE-crash.log), lädt die XAML-Ressourcen via AvaloniaXamlLoader.Load(this) im Konstruktor/Initialize-Pfad, und erzeugt im Desktop-Lifetime (Windowed-Umgebung) die MainWindow-Instanz mit zugehörigem MainWindowViewModel als DataContext, bevor der Lifetime-Start abgeschlossen wird — garantiert sauberen UI-Start trotz möglicher interner Fehler

# GatewayIDE.App/Converters.cs
Enthält den UI-ValueConverter HalfConverter : IValueConverter, der im Convert-Pfad bei gültigen double-Werten die Hälfte des Eingabewerts berechnet und gleichzeitig eine Mindestgröße von 48 px durch Math.Max(48, d * 0.5) garantiert (bei ungültigen Werten Rückfall auf 200d), während ConvertBack nicht unterstützt ist (wirft NotSupportedException); Einsatzgebiet sind Layout-Bindings, die dynamische Größen responsiv halbieren, ohne unter eine sinnvolle Mindestbreite/Höhe zu fallen.

# GatewayIDE.App/GatewayIDE.App.csproj
MSBuild-Projektdatei für die Avalonia-Desktop-App (Target net8.0, OutputType=WinExe, Nullable/ImplicitUsings aktiv, AssemblyName GatewayIDE.App) mit einem konsolidierten Paketblock für UI (Avalonia, Avalonia.Desktop, Avalonia.ReactiveUI, Avalonia.Themes.Fluent jeweils 11.1.3) und gRPC-Clientunterstützung (Grpc.Tools 2.63.0 als PrivateAssets=All, Grpc.Net.Client 2.63.0, Google.Protobuf 3.25.3), räumt zunächst alle .axaml-Einträge aus den Standard-Itemgruppen und bindet die benötigten XAMLs explizit wieder ein (App.axaml, MainWindow.axaml), inkludiert statische Assets aus Assets\**, und richtet die gRPC-Clientgenerierung gegen die Solution-Root ein (Protobuf-Include via ..\Protos\ai_service.proto, GrpcServices="Client", ProtoRoot="..\Protos"), womit das Projekt schlank bleibt (keine doppelten Paketblöcke), nur gewünschte XAMLs in den Build gelangen und bei Bedarf ein generierter gRPC-Client für ai_service.proto bereitsteht.
MSBuild-Projektdatei für die Avalonia-Desktop-App (Target net8.0, OutputType=WinExe) mit konsolidierten UI-Paketen; gRPC-Clientunterstützung ist als **optional aktivierbares Entwicklungs-Feature** hinter einem Build-Schalter vorgesehen, während REST/HTTP der produktive Transportweg bleibt

# GatewayIDE.App/MainWindow.axaml
Deklaration des Hauptfensters GatewayIDE.App.MainWindow (1280×800, Titel „Gateway IDE“) mit einem Root-Grid (Zeilen: Auto,*) für eine dunkle Top-Bar und den Inhalt, einer Topbar (44px, Spalten 44,*,Auto,Auto,Auto,Auto,Auto) mit Menü-Toggle (ToggleChatCommand) und fünf Tab-Buttons (Dashboard, Docker, KI System, Project, Blockchain jeweils über SelectTabCommand mit CommandParameter), gefolgt von einem zweispaltigen Content-Bereich: links eine Chat-Sidebar (ListBox ChatLines inkl. ItemTemplate als read-only TextBox mit Monospace-Font, plus Eingabe ChatInput mit Enter-KeyBinding → SendChatCommand), rechts der Main-Workspace als Grid mit drei Tab-Sektionen, die jeweils über IsDashboard/IsDocker/IsKiSystem sichtbar geschaltet werden; die Dashboard-Ansicht teilt sich in einen oberen Bereich (links Button-WrapPanel mit „Start Mega-Node“, rechts ein Status-Panel mit gebundenen Text/Brush-Feldern DockerDesktopStatus, DockerImageStatus, DockerContainerStatus) und ein unteres Terminal (read-only TextBox TerminalBuffer), die Docker-Ansicht bietet eine Toolbar mit Aktionsbuttons (Rebuild, Start, Stop, Remove container, Clear Logs, sowie ExpandGatewayOnlyCommand mit ExpandLabel) und ein zweiteiliges Ober-Grid (oben links DockerOutBuffer, oben rechts DockerErrBuffer, darunter vollbreit GatewayLogBuffer) plus ein Unter-Grid mit Output (ContainerIOBuffer) und mehrzeiliger Input-Box (ContainerCommand → Enter triggert ExecuteInContainerCommand), und die KI-System-Ansicht strukturiert ein 2×2-Board für T2/T4/T5/T6-Textfelder (T2Buffer, T4Buffer, T5Buffer, T6Buffer) mit Monospace-Anzeige, dazu rechte Platzhalterspalten (3 Felder) und einen unteren Platzhalterbereich; sämtliche Textboxen sind auf Wrap/Readonly/Scroll ausgelegt, viele Felder binden zusätzlich CaretIndex-Properties für Auto-Scroll, wodurch das Fenster als schlanker Mission-Control-Container für Chat, Docker-Steuerung/Logs und SOM-Thread-Telemetrie fungiert.

# GatewayIDE.App/MainWindow.axaml.cs
Code-behind des Hauptfensters mit minimalem Bootstrap: deklariert die MainWindow-Klasse (partial) im Namespace GatewayIDE.App und ruft im parameterlosen Konstruktor ausschließlich InitializeComponent() auf, wodurch die in MainWindow.axaml definierten Steuerelemente und Bindings geladen werden; keine zusätzliche UI-Logik, keine Events, dient rein der XAML-Initialisierung.

# GatewayIDE.App/Program.cs
Einstiegspunkt der Desktop-App (STAThread) und Avalonia-Bootstrap: Main(string[] args) ruft BuildAvaloniaApp().StartWithClassicDesktopLifetime(args) auf, BuildAvaloniaApp() konfiguriert den AppBuilder für App, aktiviert .UsePlatformDetect() (plattformabhängige Backends) und .LogToTrace() (einfaches Tracing-Logging), keine Custom-Args-Auswertung oder DI-Konfiguration; Ziel ist ein schlanker, plattformneutraler Start der Avalonia-Anwendung.

# GatewayIDE.App/Services/AI/AIClientService.cs
gRPC-Client-Wrapper für den vom Backend generierten AI-Service-Client (Gateway.AI.V1.AIService.AIServiceClient), erzeugt in AIClientService(string endpoint) einen GrpcChannel zu einer konfigurierten Endpoint-URL, bietet aktuell eine Beispiel-API EchoAsync(text) zur asynchronen Roundtrip-Kommunikation über den gRPC-EchoAsync-Endpoint, und übernimmt über DisposeAsync() das saubere Schließen des gRPC-Channels (kein ShutdownAsync nötig, da GrpcChannel.Dispose() genügt) — zentrale Schnittstelle, um Backend-KI-Funktionen aus dem UI anzusprechen. „gRPC-Client nur bei Entwicklungsbedarf aktivierbar; im Standard-Build inaktiv, da REST der produktive Weg ist.“
Optional aktivierbarer gRPC-Client für Entwicklungs-Tests, standardmäßig deaktiviert, da die App produktiv über die REST-Schnittstelle kommuniziert.

# GatewayIDE.App/Services/Processes/DockerService.cs
statischer High-Level-Orchestrator für Docker-Operationen, inklusive automatischer Ermittlung des Repo-Root-Pfads (FindRepoRoot), Caching der deploy-Ablage, und eines robusten, asynchronen Low-Level-Prozess-Runners (RunAsync) mit Echtzeit-Streaming von stdout/stderr (für Terminal-UI), mit Status-Abfragen zu Docker Desktop (inkl. Windows-Service-Fallback), Kommandos für Container-Lifecycle (StartGatewayAsync, StopGatewayAsync, RemoveGatewayContainerAsync, GetGatewayStatusAsync), Image-Checks (IsImageAvailableAsync), Full-Wipe & Rebuild-Pipeline (WipeAllAsync, FullRebuildAsync), Log-Streaming (TailGatewayLogsAsync) und direkter Exec-Bridge in den Container (ExecInGatewayAsync mit Bash-Shell); dient als vollständig UI-steuerbarer Control-Tower für den Gateway-Backend-Container im Entwicklungsbetrieb.

# GatewayIDE.App/Services/Processes/ProcessManager.cs
minimalistischer, generischer Prozessstarter (StartProcess(file, args, cwd?)) für externe Tools (z. B. eigene CLI-Tests), setzt WorkingDirectory, deaktiviert Shell-Ausführung, aktiviert Output-Redirect für spätere UI-Log-Integration, keine asynchrone Steuerung oder Lifecycle-Events — dient vorrangig als Legacy-Fallback gegenüber den strukturierten Docker-Routinen im DockerService.

# GatewayIDE.App/ViewModels/DelegateCommand.cs
ist ein kompakter ICommand-Wrapper zur Bindung von UI-Interaktionen, kapselt Rückrufe über _execute(object?) und optional _canExecute(object?), implementiert CanExecuteChanged und bietet RaiseCanExecuteChanged() zur UI-Aktualisierung — universeller, UI-agnostischer Command-Adapter für Buttons, Keyboard-Bindings usw., Grundlage aller ViewModel-Aktionen.

# GatewayIDE.App/ViewModels/MainWindowViewModel.cs
Haupt-ViewModel der gesamten IDE-Oberfläche: steuert Tab-Navigation und Sichtbarkeit (Dashboard, Docker, KI System), verwaltet die linke Chat-Sidebar inkl. Verlauf (ChatLines), Eingabe (ChatInput), Highlighting/Autoscroll, führt asynchrone Chat-Requests via HttpClient → POST /chat aus und verteilt Antworten kontextabhängig auf HMA-Threads (SOM→T1, SOM:INNER→T2, TASKMANAGER→T4, LIBRARIAN→T5, TRAINER→T6, RETURN→T3), trackt Systemstatus (DockerDesktopStatus, DockerImageStatus, DockerContainerStatus + Brush-Bindings), bietet umfangreiche Log-Buffer (Docker stdout/stderr + Gateway-Logs + Container-IO) und Auto-Scroll mittels Caret-Bindings, kontrolliert UI-Layout im Docker-Tab (Grid-Shrink/Expand über ExpandGatewayOnlyCommand, bindbare GridLength-Properties), enthält Full-Lifecycle-Bridges zu Docker (Rebuild, Start, Stop, Remove, Exec) über DockerService, hält Absicherungen gegen Log-Prozess-Leaks (_gatewayTailCts, _meganodeLogsProc), sowie Utility-Funktionen (Clear-Logs, Repo-Pfad-Suche, Prozess-Attach-Pipelines) — damit zentrale Mission-Control-Steuereinheit zwischen UI, System-Runtime (Docker) und HMA-Backend-Interaktion.

# protos/ai_service.proto
gRPC-Proto für den AI-Service mit einem AIService-Contract, der zwei RPC-Methoden definiert: Echo(EchoRequest) → EchoReply für synchrone Text-Roundtrips und ChatStream(stream ChatClientMsg) → stream ChatServerMsg für bidirektionale Streaming-Dialoge, jeweils mit einfachem Nachrichten-Schema (string text), dient als Grundlage zur Codegenerierung für Client (UI) und Server (Backend/Tests), wird in Visual Studio per GatewayIDE.App.csproj automatisch eingebunden und beim Build generiert. (Ich brauche später noch deinen exakten Proto-Inhalt; falls du möchtest, ergänze ich Field-IDs, Namespaces & Imports sobald vorliegend.)
„gRPC-Schema ausschließlich als Entwicklungs-Test-Double für spätere Streaming-Funktionen; Produktiv-Transport ist REST/HTTP.“
Optionales gRPC-Schema für spätere Streaming-Experimente in der Entwicklung; im normalen Betrieb nutzt das System ausschließlich REST/HTTP für Chat-Kommunikation.

# services/GatewayAI/ai_service/requirements.txt
Minimales Python-SDK-Abhängigkeitsset für eine gRPC-Service-Implementierung (grpcio==1.65.0, grpcio-tools==1.65.0), optional als Entwicklungs-/Generatorgrundlage für ai_service_pb2*.py, löst weder FastAPI-Backend noch Uvicorn aus — dedizierte, abgespeckte Runtimeschicht für Tests oder lokale Microservices.

# services/GatewayAI/ai_service/server.py
Leichter Async-gRPC-Testserver: setzt eigenen Modulpfad für generierten Code (sys.path.append(BASE)), lädt Proto-Stubs (ai_service_pb2, ai_service_pb2_grpc), implementiert AIServiceImpl mit Echo-Funktion (Echo → "echo:<input>") und bidirektionalem Chat-Stream (für jede eingehende ChatClientMsg wird "bot:<input>" zurückgegeben), startet den Server via rpc.add_AIServiceServicer_to_server auf Port 50051 (await server.start() + await wait_for_termination()), wird als eigenständiges Debug-Tool über python server.py ausgeführt — Test-Doppel für Frontend-Integration ohne komplettes Backend.
„Reiner gRPC-Testserver für lokale Experimente (Echo), kein Bestandteil des Deployments.“
Einfacher lokaler gRPC-Testserver für Entwicklungszwecke (Echo-Demo) und keine Komponente des Deployments oder des produktiven Datenpfads.




